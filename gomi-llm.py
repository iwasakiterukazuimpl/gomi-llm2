# rag_sample.py

import os
import json
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

# APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# â‘  Webãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def scrape_text(url):
    """æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°"""
    response = requests.get(url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, "html.parser")
    
    garbage_info = []
    
    # 50éŸ³é †ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆh2ã¨h3ã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
    sections = soup.find_all(['h2', 'h3'], class_=None)
    for section in sections:
        section_title = section.get_text(strip=True)
        if not section_title:
            continue
            
        table = section.find_next('table')
        if not table:
            continue
            
        rows = table.find_all('tr')
        for row in rows:
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 3:
                item = extract_cell_text(cells[0])
                category = extract_cell_text(cells[1])
                fee = extract_cell_text(cells[2])
                if fee == "-":
                    fee = "ç„¡æ–™"
                
                note = ""
                if len(cells) > 3:
                    note = extract_cell_text(cells[3])
                
                if item and category:
                    info = {
                        "å“ç›®": str(item),
                        "åˆ†åˆ¥åŒºåˆ†": str(category),
                        "æ‰‹æ•°æ–™": str(fee),
                        "å‚™è€ƒ": str(note) if note and note != "&nbsp;" else ""
                    }
                    garbage_info.append(info)
    
    if not garbage_info:
        return soup.get_text()
    
    garbage_info = clean_html_from_items(garbage_info)
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open('garbage_data.json', 'w', encoding='utf-8') as f:
        json.dump(garbage_info, f, ensure_ascii=False, indent=2)
    
    # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã‚‚ä¿å­˜
    with open('garbage_data.txt', 'w', encoding='utf-8') as f:
        for info in garbage_info:
            f.write(f"å“ç›®: {info['å“ç›®']} | åˆ†åˆ¥åŒºåˆ†: {info['åˆ†åˆ¥åŒºåˆ†']} | æ‰‹æ•°æ–™: {info['æ‰‹æ•°æ–™']}")
            if info['å‚™è€ƒ']:
                f.write(f" | å‚™è€ƒ: {info['å‚™è€ƒ']}")
            f.write('\n')
    
    return '\n'.join([f"å“ç›®: {info['å“ç›®']} | åˆ†åˆ¥åŒºåˆ†: {info['åˆ†åˆ¥åŒºåˆ†']} | æ‰‹æ•°æ–™: {info['æ‰‹æ•°æ–™']}" + 
                     (f" | å‚™è€ƒ: {info['å‚™è€ƒ']}" if info['å‚™è€ƒ'] else "") for info in garbage_info])

def extract_cell_text(cell):
    """ã‚»ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹æ”¹å–„ã•ã‚ŒãŸé–¢æ•°"""
    if '<' in str(cell) and '>' in str(cell):
        return BeautifulSoup(str(cell), 'html.parser').get_text(strip=True)
    
    return cell.get_text(strip=True)

def clean_html_from_items(data):
    """HTMLã‚¿ã‚°ã‚’å“ç›®åã‹ã‚‰é™¤å»"""
    for item in data:
        for key in ['å“ç›®', 'åˆ†åˆ¥åŒºåˆ†', 'æ‰‹æ•°æ–™', 'å‚™è€ƒ']:
            if key in item and '<' in str(item[key]) and '>' in str(item[key]):
                soup = BeautifulSoup(str(item[key]), 'html.parser')
                item[key] = soup.get_text(strip=True)
    
    return data

# â‘¡ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
def chunk_text(text, chunk_size=1200, chunk_overlap=150):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_text(text)

# â‘¢ Embeddingã¨FAISSã§ä¿å­˜
def create_vector_store(chunks):
    embeddings = OpenAIEmbeddings()
    return FAISS.from_texts(chunks, embeddings)

from langchain.prompts import PromptTemplate

template = """ã‚ãªãŸã¯æœ­å¹Œå¸‚ã®ã‚´ãƒŸåˆ†åˆ¥ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã‹ã¤ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€é¡ä¼¼ã™ã‚‹å“ç›®ã‚’æ¢ã—ã¦å‚è€ƒæƒ…å ±ã¨ã—ã¦æä¾›ã—ã¦ãã ã•ã„ã€‚
ä¾‹ãˆã°ã€ã€Œç´™ãƒ‘ãƒƒã‚¯ã€ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€Œç´™è£½å®¹å™¨ã€ã‚„ã€Œã‚«ãƒ¼ãƒˆãƒ³ã€ãªã©ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

æƒ…å ±:
{context}

è³ªå•: {question}

å›ç­”:"""

CUSTOM_PROMPT_TEMPLATE = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# â‘£ è³ªå•ã‚’å‡¦ç†ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
def ask_question(vectorstore, query):
    query = preprocess_query(query)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0.3, model="gpt-4"),
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={
            "prompt": CUSTOM_PROMPT_TEMPLATE
        }
    )
    return qa.run(query)

def preprocess_query(query):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã®å‰å‡¦ç†"""
    synonyms = {
        "ç‰›ä¹³ãƒ‘ãƒƒã‚¯": ["ç´™ãƒ‘ãƒƒã‚¯", "ãƒŸãƒ«ã‚¯ãƒ‘ãƒƒã‚¯", "é£²æ–™ãƒ‘ãƒƒã‚¯"],
        "ç´™ãƒ‘ãƒƒã‚¯": ["ç‰›ä¹³ãƒ‘ãƒƒã‚¯", "ãƒŸãƒ«ã‚¯ãƒ‘ãƒƒã‚¯", "é£²æ–™ãƒ‘ãƒƒã‚¯"]
    }
    
    for key, values in synonyms.items():
        if key in query:
            return query
        for value in values:
            if value in query:
                return query.replace(value, key)
    
    return query

# å®Ÿè¡Œ
if __name__ == "__main__":
    url = "https://www.city.sapporo.jp/seiso/bunbetsu/index.html"
    raw_text = scrape_text(url)
    chunks = chunk_text(raw_text)
    vectorstore = create_vector_store(chunks)
    
    print("âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ã€‚è³ªå•ã‚’ã©ã†ãï¼\n")

    while True:
        query = input("ğŸ” è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ç‰›ä¹³ãƒ‘ãƒƒã‚¯ã¯ä½•ã”ã¿ï¼Ÿï¼‰ï¼š")
        if query in ["exit", "quit", ""]:
            break
        answer = ask_question(vectorstore, query)
        print("ğŸ§  å›ç­”:", answer)
