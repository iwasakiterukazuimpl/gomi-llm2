# rag_sample.py

import os
import json
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

# APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# â‘  Webãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def scrape_text(url):
    # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æŒ‡å®šã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    response = requests.get(url)
    response.encoding = 'utf-8'  # æ˜ç¤ºçš„ã«UTF-8ã‚’æŒ‡å®š
    soup = BeautifulSoup(response.text, "html.parser")
    
    # ã‚´ãƒŸåˆ†åˆ¥æƒ…å ±ã‚’æŠ½å‡º
    garbage_info = []
    
    # 50éŸ³é †ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—
    sections = soup.find_all('h2', class_=None)
    for section in sections:
        section_title = section.get_text(strip=True)
        if not section_title:  # ç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
            
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
        table = section.find_next('table')
        if not table:
            continue
            
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’å–å¾—
        rows = table.find_all('tr')
        for row in rows:
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 3:  # å“ç›®ã€åˆ†åˆ¥åŒºåˆ†ã€æ‰‹æ•°æ–™ã®åˆ—ãŒã‚ã‚‹å ´åˆ
                # å“ç›®ï¼ˆå¤ªå­—ã‚¿ã‚°ã‚’å«ã‚€å ´åˆã‚‚è€ƒæ…®ï¼‰
                item_cell = cells[0]
                strong_tag = item_cell.find('strong')
                item = strong_tag.get_text(strip=True) if strong_tag else item_cell.get_text(strip=True)
                
                # åˆ†åˆ¥åŒºåˆ†ï¼ˆãƒªãƒ³ã‚¯ã‚’å«ã‚€å ´åˆã‚‚è€ƒæ…®ï¼‰
                category_cell = cells[1]
                category = category_cell.get_text(strip=True)
                category_link = category_cell.find('a')
                if category_link:
                    category = f"{category_link.get_text(strip=True)}"
                
                # æ‰‹æ•°æ–™
                fee = cells[2].get_text(strip=True)
                if fee == "-":
                    fee = "ç„¡æ–™"
                
                # å‚™è€ƒï¼ˆãƒªãƒ³ã‚¯ã€æ®µè½ã€ãƒªã‚¹ãƒˆã‚’å«ã‚€å ´åˆã‚‚è€ƒæ…®ï¼‰
                note = ""
                if len(cells) > 3:
                    note_cell = cells[3]
                    # ãƒªã‚¹ãƒˆè¦ç´ ã‚’å‡¦ç†
                    lists = note_cell.find_all(['ul', 'ol'])
                    if lists:
                        list_items = []
                        for list_elem in lists:
                            for item in list_elem.find_all('li'):
                                # ãƒªã‚¹ãƒˆé …ç›®å†…ã®ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
                                links = item.find_all('a')
                                if links:
                                    for link in links:
                                        link_text = link.get_text(strip=True)
                                        item_text = item.get_text(strip=True)
                                        # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒã—ãªãŒã‚‰ã€é‡è¤‡ã‚’é¿ã‘ã‚‹
                                        if link_text in item_text:
                                            list_items.append(item_text)
                                        else:
                                            list_items.append(f"{item_text} ({link_text})")
                                else:
                                    list_items.append(item.get_text(strip=True))
                        note = " ".join(list_items)
                    else:
                        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
                        note_parts = []
                        for element in note_cell.stripped_strings:
                            note_parts.append(str(element))  # ç¢ºå®Ÿã«æ–‡å­—åˆ—ã«å¤‰æ›
                        note = " ".join(note_parts)
                
                if item and category:  # ç©ºã®è¡Œã¯é™¤å¤–
                    info = {
                        "å“ç›®": str(item),  # ç¢ºå®Ÿã«æ–‡å­—åˆ—ã«å¤‰æ›
                        "åˆ†åˆ¥åŒºåˆ†": str(category),  # ç¢ºå®Ÿã«æ–‡å­—åˆ—ã«å¤‰æ›
                        "æ‰‹æ•°æ–™": str(fee),  # ç¢ºå®Ÿã«æ–‡å­—åˆ—ã«å¤‰æ›
                        "å‚™è€ƒ": str(note) if note and note != "&nbsp;" else ""  # ç¢ºå®Ÿã«æ–‡å­—åˆ—ã«å¤‰æ›
                    }
                    garbage_info.append(info)
    
    # ã‚´ãƒŸåˆ†åˆ¥æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å¾“æ¥ã®æ–¹æ³•ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    if not garbage_info:
        return soup.get_text()
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆUTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
    with open('garbage_data.json', 'w', encoding='utf-8') as f:
        json.dump(garbage_info, f, ensure_ascii=False, indent=2)
    
    # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    with open('garbage_data.txt', 'w', encoding='utf-8') as f:
        for info in garbage_info:
            f.write(f"å“ç›®: {info['å“ç›®']} | åˆ†åˆ¥åŒºåˆ†: {info['åˆ†åˆ¥åŒºåˆ†']} | æ‰‹æ•°æ–™: {info['æ‰‹æ•°æ–™']}")
            if info['å‚™è€ƒ']:
                f.write(f" | å‚™è€ƒ: {info['å‚™è€ƒ']}")
            f.write('\n')
    
    return '\n'.join([f"å“ç›®: {info['å“ç›®']} | åˆ†åˆ¥åŒºåˆ†: {info['åˆ†åˆ¥åŒºåˆ†']} | æ‰‹æ•°æ–™: {info['æ‰‹æ•°æ–™']}" + 
                     (f" | å‚™è€ƒ: {info['å‚™è€ƒ']}" if info['å‚™è€ƒ'] else "") for info in garbage_info])

# â‘¡ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
def chunk_text(text, chunk_size=1200, chunk_overlap=150):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_text(text)

# â‘¢ Embeddingã¨FAISSã§ä¿å­˜
def create_vector_store(chunks):
    embeddings = OpenAIEmbeddings()
    return FAISS.from_texts(chunks, embeddings)

from langchain.prompts import PromptTemplate

template = """ã‚ãªãŸã¯æœ­å¹Œå¸‚ã®ã‚´ãƒŸåˆ†åˆ¥ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã‹ã¤ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

æƒ…å ±:
{context}

è³ªå•: {question}

å›ç­”:"""

CUSTOM_PROMPT_TEMPLATE = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# â‘£ è³ªå•ã‚’å‡¦ç†ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
def ask_question(vectorstore, query):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0.3, model="gpt-4"),
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={
            "prompt": CUSTOM_PROMPT_TEMPLATE
        }
    )
    return qa.run(query)

# å®Ÿè¡Œ
if __name__ == "__main__":
    url = "https://www.city.sapporo.jp/seiso/bunbetsu/index.html"
    raw_text = scrape_text(url)
    chunks = chunk_text(raw_text)
    vectorstore = create_vector_store(chunks)

    print("âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ã€‚è³ªå•ã‚’ã©ã†ãï¼\n")

    while True:
        query = input("ğŸ” è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ç‰›ä¹³ãƒ‘ãƒƒã‚¯ã¯ä½•ã”ã¿ï¼Ÿï¼‰ï¼š")
        if query in ["exit", "quit", ""]:
            break
        answer = ask_question(vectorstore, query)
        print("ğŸ§  å›ç­”:", answer)
